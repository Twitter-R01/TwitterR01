{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings for tweets\n",
    "Refer: https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c\n",
    "\n",
    "Train embeddings using tweets from January - July 2018. \n",
    "\n",
    "Total unique tweets = 1899851\n",
    "\n",
    "Embeddings are trained using both Word2Vec CBOW and Skipgram models for unigrams and bigrams (phrases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving all unique tweets from Jan-July 2018 in separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(file):\n",
    "    df = pd.read_table(file)\n",
    "    tweets = df['text'].tolist()\n",
    "    tweets = list(set(tweets))\n",
    "    tweets_all.extend(tweets)\n",
    "    print(len(tweets_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180227000000_data.tsv\n",
      "9167\n",
      "20180320000000_data.tsv\n",
      "9708\n",
      "20180625000000_data.tsv\n",
      "21732\n",
      "20180130000000_data.tsv\n",
      "32411\n",
      "20180722000000_data.tsv\n",
      "42632\n",
      "20180510000000_data.tsv\n",
      "53602\n",
      "20180205000000_data.tsv\n",
      "62284\n",
      "20180409000000_data.tsv\n",
      "72775\n",
      "20180417000000_data.tsv\n",
      "83511\n",
      "20180112000000_data.tsv\n",
      "92292\n",
      "20180607000000_data.tsv\n",
      "105784\n",
      "20180619000000_data.tsv\n",
      "117650\n",
      "20180725000000_data.tsv\n",
      "131709\n",
      "20180622000000_data.tsv\n",
      "145191\n",
      "20180320005116_data.tsv\n",
      "155809\n",
      "20180129000000_data.tsv\n",
      "165099\n",
      "20180327000000_data.tsv\n",
      "176204\n",
      "20180220000000_data.tsv\n",
      "185030\n",
      "20180707000000_data.tsv\n",
      "196361\n",
      "20180719000000_data.tsv\n",
      "209369\n",
      "20180115000000_data.tsv\n",
      "217165\n",
      "20180402131247_data.tsv\n",
      "223281\n",
      "20180410000000_data.tsv\n",
      "227713\n",
      "20180509000000_data.tsv\n",
      "238480\n",
      "20180517000000_data.tsv\n",
      "253838\n",
      "20180319162233_data.tsv\n",
      "253839\n",
      "20180202000000_data.tsv\n",
      "263137\n",
      "20180617000000_data.tsv\n",
      "273755\n",
      "20180609000000_data.tsv\n",
      "283634\n",
      "20180102000000_data.tsv\n",
      "284472\n",
      "20180710000000_data.tsv\n",
      "299773\n",
      "20180215000000_data.tsv\n",
      "307709\n",
      "20180419000000_data.tsv\n",
      "318844\n",
      "20180407000000_data.tsv\n",
      "328637\n",
      "20180120000000_data.tsv\n",
      "336572\n",
      "20180522000000_data.tsv\n",
      "342671\n",
      "20180330000000_data.tsv\n",
      "344116\n",
      "20180425000000_data.tsv\n",
      "358388\n",
      "20180212000000_data.tsv\n",
      "367825\n",
      "20180519000000_data.tsv\n",
      "376700\n",
      "20180507000000_data.tsv\n",
      "388105\n",
      "20180717000000_data.tsv\n",
      "401074\n",
      "20180709000000_data.tsv\n",
      "415437\n",
      "20180610000000_data.tsv\n",
      "425450\n",
      "20180105000000_data.tsv\n",
      "427459\n",
      "20180329000000_data.tsv\n",
      "439849\n",
      "20180422000000_data.tsv\n",
      "448511\n",
      "20180525000000_data.tsv\n",
      "458878\n",
      "20180127000000_data.tsv\n",
      "467368\n",
      "20180620200643_data.tsv\n",
      "470093\n",
      "20180515160944_data.tsv\n",
      "474772\n",
      "20180620000000_data.tsv\n",
      "485178\n",
      "20180727000000_data.tsv\n",
      "497889\n",
      "20180222000000_data.tsv\n",
      "506815\n",
      "20180529000000_data.tsv\n",
      "517749\n",
      "20180430000000_data.tsv\n",
      "529148\n",
      "20180325000000_data.tsv\n",
      "538866\n",
      "20180109000000_data.tsv\n",
      "539749\n",
      "20180117000000_data.tsv\n",
      "550559\n",
      "20180602000000_data.tsv\n",
      "560661\n",
      "20180705000000_data.tsv\n",
      "573007\n",
      "20180412000000_data.tsv\n",
      "583698\n",
      "20180429000000_data.tsv\n",
      "593320\n",
      "20180628170405_data.tsv\n",
      "597218\n",
      "20180322000000_data.tsv\n",
      "607805\n",
      "20180225000000_data.tsv\n",
      "615794\n",
      "20180530000000_data.tsv\n",
      "628443\n",
      "20180720000000_data.tsv\n",
      "641620\n",
      "20180415000000_data.tsv\n",
      "650557\n",
      "20180207000000_data.tsv\n",
      "661673\n",
      "20180219000000_data.tsv\n",
      "670801\n",
      "20180702000000_data.tsv\n",
      "682502\n",
      "20180110000000_data.tsv\n",
      "690150\n",
      "20180605000000_data.tsv\n",
      "701664\n",
      "20180210000000_data.tsv\n",
      "710412\n",
      "20180505000000_data.tsv\n",
      "719311\n",
      "20180612000000_data.tsv\n",
      "734284\n",
      "20180119000000_data.tsv\n",
      "743939\n",
      "20180107000000_data.tsv\n",
      "749499\n",
      "20180715000000_data.tsv\n",
      "760500\n",
      "20180527000000_data.tsv\n",
      "770830\n",
      "20180420000000_data.tsv\n",
      "782417\n",
      "20180125000000_data.tsv\n",
      "791734\n",
      "20180630000000_data.tsv\n",
      "802447\n",
      "20180729000000_data.tsv\n",
      "812670\n",
      "20180712000000_data.tsv\n",
      "825185\n",
      "20180615000000_data.tsv\n",
      "839494\n",
      "20180405000000_data.tsv\n",
      "849391\n",
      "20180217000000_data.tsv\n",
      "856970\n",
      "20180209000000_data.tsv\n",
      "869775\n",
      "20180502000000_data.tsv\n",
      "880959\n",
      "20180730000000_data.tsv\n",
      "894716\n",
      "20180122000000_data.tsv\n",
      "905187\n",
      "20180629000000_data.tsv\n",
      "917358\n",
      "20180427000000_data.tsv\n",
      "928696\n",
      "20180520000000_data.tsv\n",
      "937310\n",
      "20180410113218_data.tsv\n",
      "937362\n",
      "20180211000000_data.tsv\n",
      "945621\n",
      "20180504000000_data.tsv\n",
      "955955\n",
      "20180403000000_data.tsv\n",
      "967015\n",
      "20180402130737_data.tsv\n",
      "967028\n",
      "20180613000000_data.tsv\n",
      "981130\n",
      "20180106000000_data.tsv\n",
      "984307\n",
      "20180714000000_data.tsv\n",
      "996177\n",
      "20180526000000_data.tsv\n",
      "1007220\n",
      "20180421000000_data.tsv\n",
      "1015816\n",
      "20180124000000_data.tsv\n",
      "1027254\n",
      "20180728000000_data.tsv\n",
      "1038238\n",
      "20180713000000_data.tsv\n",
      "1051910\n",
      "20180614000000_data.tsv\n",
      "1066396\n",
      "20180101000000_data.tsv\n",
      "1068421\n",
      "20180404000000_data.tsv\n",
      "1068824\n",
      "20180216000000_data.tsv\n",
      "1076707\n",
      "20180208000000_data.tsv\n",
      "1089670\n",
      "20180503000000_data.tsv\n",
      "1100154\n",
      "20180731000000_data.tsv\n",
      "1114177\n",
      "20180123000000_data.tsv\n",
      "1125367\n",
      "20180426000000_data.tsv\n",
      "1138526\n",
      "20180521000000_data.tsv\n",
      "1146265\n",
      "20180621000000_data.tsv\n",
      "1160098\n",
      "20180726000000_data.tsv\n",
      "1174195\n",
      "20180223000000_data.tsv\n",
      "1183008\n",
      "20180528000000_data.tsv\n",
      "1194117\n",
      "20180324000000_data.tsv\n",
      "1203471\n",
      "20180108000000_data.tsv\n",
      "1210650\n",
      "20180116000000_data.tsv\n",
      "1219415\n",
      "20180603000000_data.tsv\n",
      "1229743\n",
      "20180704000000_data.tsv\n",
      "1243259\n",
      "20180319214620_data.tsv\n",
      "1244494\n",
      "20180413000000_data.tsv\n",
      "1254648\n",
      "20180428000000_data.tsv\n",
      "1264091\n",
      "20180323000000_data.tsv\n",
      "1274634\n",
      "20180224000000_data.tsv\n",
      "1281802\n",
      "20180531000000_data.tsv\n",
      "1297195\n",
      "20180721000000_data.tsv\n",
      "1308393\n",
      "20180626000000_data.tsv\n",
      "1312657\n",
      "20180414000000_data.tsv\n",
      "1322270\n",
      "20180206000000_data.tsv\n",
      "1333386\n",
      "20180218000000_data.tsv\n",
      "1341254\n",
      "20180703000000_data.tsv\n",
      "1353544\n",
      "20180522133649_data.tsv\n",
      "1359342\n",
      "20180111000000_data.tsv\n",
      "1367153\n",
      "20180604000000_data.tsv\n",
      "1377909\n",
      "20180616000000_data.tsv\n",
      "1388616\n",
      "20180608000000_data.tsv\n",
      "1398879\n",
      "20180711000000_data.tsv\n",
      "1412445\n",
      "20180214000000_data.tsv\n",
      "1421310\n",
      "20180410113545_data.tsv\n",
      "1427501\n",
      "20180501000000_data.tsv\n",
      "1438869\n",
      "20180418000000_data.tsv\n",
      "1450722\n",
      "20180406000000_data.tsv\n",
      "1460898\n",
      "20180121000000_data.tsv\n",
      "1472658\n",
      "20180523000000_data.tsv\n",
      "1483344\n",
      "20180228000000_data.tsv\n",
      "1488246\n",
      "20180424000000_data.tsv\n",
      "1501476\n",
      "20180213000000_data.tsv\n",
      "1510040\n",
      "20180404105623_data.tsv\n",
      "1516639\n",
      "20180518000000_data.tsv\n",
      "1529019\n",
      "20180521221640_data.tsv\n",
      "1530092\n",
      "20180506000000_data.tsv\n",
      "1539013\n",
      "20180716000000_data.tsv\n",
      "1552083\n",
      "20180708000000_data.tsv\n",
      "1564635\n",
      "20180118000001_data.tsv\n",
      "1574878\n",
      "20180611000000_data.tsv\n",
      "1588003\n",
      "20180104000000_data.tsv\n",
      "1588676\n",
      "20180328000000_data.tsv\n",
      "1600907\n",
      "20180423000000_data.tsv\n",
      "1611204\n",
      "20180524000000_data.tsv\n",
      "1622434\n",
      "20180126000000_data.tsv\n",
      "1631587\n",
      "20180226000000_data.tsv\n",
      "1640268\n",
      "20180321000000_data.tsv\n",
      "1651069\n",
      "20180624000000_data.tsv\n",
      "1661041\n",
      "20180131000000_data.tsv\n",
      "1670657\n",
      "20180723000000_data.tsv\n",
      "1683027\n",
      "20180511000000_data.tsv\n",
      "1683263\n",
      "20180204000000_data.tsv\n",
      "1691018\n",
      "20180408000000_data.tsv\n",
      "1700251\n",
      "20180416000000_data.tsv\n",
      "1710125\n",
      "20180113000000_data.tsv\n",
      "1717660\n",
      "20180606000000_data.tsv\n",
      "1730807\n",
      "20180618000000_data.tsv\n",
      "1741182\n",
      "20180701000000_data.tsv\n",
      "1751380\n",
      "20180410110047_data.tsv\n",
      "1751707\n",
      "20180724000000_data.tsv\n",
      "1760559\n",
      "20180623000000_data.tsv\n",
      "1771239\n",
      "20180128000000_data.tsv\n",
      "1778996\n",
      "20180326000000_data.tsv\n",
      "1789572\n",
      "20180221000000_data.tsv\n",
      "1798832\n",
      "20180706000000_data.tsv\n",
      "1811855\n",
      "20180718000000_data.tsv\n",
      "1825220\n",
      "20180114000000_data.tsv\n",
      "1832674\n",
      "20180601000000_data.tsv\n",
      "1846401\n",
      "20180411000000_data.tsv\n",
      "1856878\n",
      "20180508000000_data.tsv\n",
      "1867540\n",
      "20180201000001_data.tsv\n",
      "1877686\n",
      "20180516000000_data.tsv\n",
      "1891529\n",
      "20180203000000_data.tsv\n",
      "1899851\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = '/pylon5/be5fpap/sanyabt/RITHM/parser_out/jan_july_2018/'\n",
    "tweets_all = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".tsv\"):\n",
    "        print(filename)\n",
    "        get_tweets(os.path.join(directory, filename))\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pylon5/be5fpap/sanyabt/RITHM/parser_out/jan_july_2018/unique_tweets.pickle', 'wb') as file_o:\n",
    "    pickle.dump(tweets_all, file_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean tweets for embeddings\n",
    "\n",
    "1. Remove URL's, mentions and unicodes\n",
    "2. Remove HTML characters and special symbols\n",
    "3. Lowercase text\n",
    "4. Expand contractions\n",
    "5. Remove numbers\n",
    "6. Remove punctuation symbols\n",
    "7. Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "contractions_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
    "tzer = tokenize.RegexpTokenizer(r'[A-Za-z_]+')   \n",
    "\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "pat3 = r'www\\.[^ ]+'\n",
    "pat4 = r'\\\\u[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2, pat3, pat4))\n",
    "re_pat = re.compile(combined_pat)\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text().lower()      # lowercase the whole thing here\n",
    "    bomgone = souped.replace('ï¿½', ' ')\n",
    "    re_cleaned = re_pat.sub(' ', bomgone)\n",
    "    neg_handled = neg_pattern.sub(lambda x: contractions_dict[x.group()], re_cleaned)\n",
    "    tokenized = tzer.tokenize(neg_handled)\n",
    "    return \" \".join(tokenized)\n",
    "\n",
    "def remove_underscores(text):\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        if '_' in word:\n",
    "            if '_emoj' not in word:\n",
    "                words[words.index(word)] = ''\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' # ecig # vapecommunity # vapeon # vapefam FIRST LOOK - Stentorian Basilisk: Please note this is NOT a review -  this is a first look at the product only. Review will be up later this month. The Late Late UK Vape Show is ... https://t.co/fkbpuhBpah # VapingWithVic # Vape # vapingAuction https://t.co/FM5F9zlumV ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Times Vape / TenaciousTX Vapes Dreamer Review - A solid mech mod...: The Times Vape and TenaciousTXVapes Dreamer mech has been creating a lot of waves over the past couple of months. ... https://t.co/EMT9NskKYJ # VapingWithVic # Vape # vapingdays https://t.co/gtWkPg5hzM ',\n",
       " ' all of my co workers including the manager r outside vaping and im alone at front counter ',\n",
       " \" When I 'm too old to smoke I 'm gonna start vaping. Until then -  my lungs better do their job. \",\n",
       " ' Kansas man sues vape shop for e-cigarette battery explosion. In other words -  man attempts to blame his stupidity on others -  instead of owning it. Lawyer vultures looking for a dollar. https://t.co/i5qrzjWVoR ',\n",
       " ' Vapour Freaks: Hannibal On Ice Juice Review!!   _emoj_en_dash_\\n Empire Vape Co https://t.co/5kOaTlf6mi<br><br> https://t.co/giWyoQ36iP ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Wotofo Serpent SMM Review + Coiling and wicking - Now the single coil RTA war begins...: The Wotofo Serpent SMM (Suck My Mod) is the latest tank from Wotofo which carries on the ... https://t.co/kHHpUiR0md # VapingWithVic # Vape # vapingAuction https://t.co/NDXwnzFfLQ ',\n",
       " ' All those vape gods @ my school are shook over this https://t.co/rAcvYOczpB ',\n",
       " ' VAPE TIL YOU DIE https://t.co/R9ygumdQPN ',\n",
       " \" FYI: # ecig # vapecommunity # vapeon # vapefam The Devil Vaper's Best & Worst of 2017!: A video rounding up the year and giving my personal best and worst of 2017. Please refer to the timestamps below to jump to your ... https://t.co/jar5r4nWTF # TheDevilVaper # Vape # vapingdays https://t.co/UhOlvNuEZx \",\n",
       " ' holy fuck i underestimated the juul power hducuxjslw ',\n",
       " \" My mom just caught me charging my vape lmao FUUUUCK I lied straight to her face and said it wasn't mine   _emoj_joy_ \\n  _emoj_sob_ \\n \",\n",
       " \" man i got these pods in the mail Wednesday and I 'm already almost done with my third pod no one ask me touch my juul bc I will sucker punch the shit out of you  _newline_  _newline_ and don 't touch me either \",\n",
       " ' So -  @FDATobacco & @SGottliebFDA -  any chance @US_FDA will be moving to correct the similar misconception (that you helped create) that vaping (etc) is worthless for risk reduction except when immediate smoking abstinence occurs? _newline_ https://t.co/remJuQ9bxk ',\n",
       " ' FYI: # ecig # vapecommunity # vapeon # vapefam Bryn\\'s Special Sauce - T\\\\xc9: A review of Bryn\\'s Special Sauce - T\\\\xc9 \"Fresh and candied citrus fruits -  and rangpur are blended with notes of Jasmine -  Darjeeling and Assam and ... https://t.co/z29tn6T8Rf # TheDevilVaper # Vape # vapingAuction https://t.co/J7ohdBtdRA ',\n",
       " \" 46281: It's 4:20 PM (16:20) in 32 timezones. # rootsreggae # vapes _newline_  _newline_ https://t.co/LHsl3V0qeG \",\n",
       " \" Can I buy a juul pod from someone lmao I don 't get paid till Friday -  and I can 't be spendin freakin $20 on a whole pack   _emoj_joy_ \\n  _emoj_joy_ \\n  _emoj_joy_ \\n  _emoj_joy_ \\n  _emoj_joy_ \\n \",\n",
       " ' I dated a guy who looked like him https://t.co/5QRjzXvUDB ',\n",
       " ' Just watched a kid blow a massive vape cloud while hover boarding at mach speed across campus we are the pinnacle of technology here at RIT ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Watts UP?! Ep 125 - TPD aftermath -  Expo -  Medusa Reborn + Giveaway!: In this weeks Watts Up... the aftermath of May 20th -  Vaper Expo UK -  a LOT of subscriber and viewer mail -  ... https://t.co/lpFURXEnG4 # VapingWithVic # Vape # vapingAuction https://t.co/fbWs9LCEdC ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Watts UP?! - Ep 144 - Expo news -  25k subs + first looks and BIG BOX GIVEAWAY: In this weeks Watts Up -  the old expo problems pop up again -  an interesting vlog from Chris Empire Vape ... https://t.co/NeIJVbREgJ # VapingWithVic # Vape # vapingdays https://t.co/GDoiWX8sPm ',\n",
       " ' Find all your # ecig / # vape supplies at @MKEVape on @BradystreetBID in # Milwaukee! Helpful -  knowledgeable staff. https://t.co/0VTLg2HOtX https://t.co/V2MG1sYpfe ',\n",
       " \" FYI: # ecig # vapecommunity # vapeon # vapefam The Devil's VLOG Episode 9 + # AskTheDevil: The Devil's VLOG EP 9! *TIME STAMPS DOWN BELOW* In this episode I cover what I have been vaping on this week -  what products have ... https://t.co/eGMbcEW47S # TheDevilVaper # Vape # vapingAuction https://t.co/zwWv4BUTDg \",\n",
       " '   _emoj_black_right_pointing_triangle_ \\n @Magnetic_Mag: Puff in Style with the Puffco Plus - The Puffco Plus vape pen has a sleek -  simple -  discrete design so you can inconspicuously vape concentrates on the go in style. No torches. No mess. Expect a streamlined experience. The pen comes in ... https://t.co/04urjiJVeA ',\n",
       " ' WHOLESALE VJUICE _newline_ # ecigs # vapecommunity # vapefamily # vapenation # vapeshop # VapeTricks # cloudchaser # vapelife # vapeporn # ecigs # vapenation # Vape # cloudchaser # vapelife # JBRT18 _newline_ WHOLESALE SALES FAST DPD DELIVERY _newline_ e mail sms@jewellerybank.co.uk OR CALL PAUL ON 07801 070 726 https://t.co/hTOvGL0xgS ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Vaptio C2 Kit Review - One of the larger pen style kits: In today\\'s review -  we are having a look at the Vaptio C2 Kit. The latest of the \"pen style\" kits from Vaptio -  the CII takes ... https://t.co/LoeHIVqGJm # VapingWithVic # Vape # vapingdays https://t.co/kffaqIcA4C ',\n",
       " ' - plays ultimate frisbee   _newline_ - suspended for vaping in bathroom   _newline_ - works at little Caesars   _newline_ - wears mandals to school https://t.co/nkoxwXBner ',\n",
       " ' @iJustBall24_7 Found a juul charger on the ground in collegetown this weekend. Life has been good. ',\n",
       " ' That moment you decide to run low wattage and you need the best to run with it so u top it off with an Elite V2 from Armageddon and some of the best juice Fluffer from Liquid Sky  _newline_ # vapes # vapenation # vapelife # vapefamily # vapefam # vapes # VapeOn # vapedaily # vapelyfe https://t.co/srWNi9l4fe ',\n",
       " \" # ecig # vapecommunity # vapeon # vapefam Dead Rabbit RDA Review - Very impressive RDA (great for squonking): In today's review -  we are having a look at the Dead Rabbit RDA by Hellvape and designed by Heathen. In a market ... https://t.co/On238YR8xM # VapingWithVic # Vape # vapingtrain https://t.co/sS6p6JktBG \",\n",
       " ' Vapes are so loud today more than eggs -  whats going on? _newline_  _newline_ # iHeartAwards # bestfanarmy # BTSARMY ',\n",
       " \" @TobaccoPrev maybe kids are vaping.  It's that or drugs and alcohol   _emoj_thinking_face_\\n # whichwouldiprefer \",\n",
       " \" @Ashley_Hex It varies by how much pain your in I vape about 400mg just to be on the safe side but it ranges from 25mg-1000mg and price varies too I pay around $75 a month but I think I'm gonna up my level to 600mg but just like vape juice the got different flavors and all \",\n",
       " \" Two bloody Mary 's down -  moderately buzzed and discreetly vaping in th airport toilets  \\\\u0001f919  _emoj_fitzpatrick_type_1_2_\\n \",\n",
       " \" Sorry - Vaping and edibles ain't good enough................. _newline_  _newline_ https://t.co/ostMjwnVVG \",\n",
       " \" it all started when da vinci-chan's shop got a vape section. at first it was just a few -  like robin hood and kintoki. now over half of the servants vape. the c-block is so filled with vape mist i cant go there without choking to death \",\n",
       " \" I accidentally posted a video of me hitting a vape pen and my fam is going to think it's wax   _emoj_sob_ \\n  _emoj_sob_ \\n I done fucked up \",\n",
       " ' i got told to draw marie vaping _newline_ i suck at expressions lmaooooo https://t.co/mZXxFwwvdu ',\n",
       " ' Here are the witnesses... # vaping # vapinguk https://t.co/c4NIgtbOG9 ',\n",
       " ' CAN U HOLD MY JUUL ',\n",
       " \" no -  i won 't fill up your kush vape for you. ask someone else  _emoj_face_palm_\\n  _emoj_fitzpatrick_type_3\\n   _emoj_male_sign_ \\n \\\\ufe0f \",\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Vape Mail Magazine + E-Liquid Samples - July Subscription Package: Having a vape on this months July e-liquid samples from the Vape Mail Magazine subscription....  \\\\u2550 \\\\u2563 Vape Mail ... https://t.co/1wwmTlQkBq # VapingWithVic # Vape # vapingAuction https://t.co/XLFcsSuKzp ',\n",
       " \" # ecig # vapecommunity # vapeon # vapefam DovPo M VV Mod Review - DovPo go back to basics...: In today's review -  we are having a look at the DovPo M VV mod -  a potentiometer based mod from Dovpo. The version I have here is ... https://t.co/O4lW7CQRG0 # VapingWithVic # Vape # vapingAuction https://t.co/z0z4RW6PWC \",\n",
       " ' Russia May Take A Pro-Vaping Stance _newline_ By Jim McDonald @whycherrywhy  _newline_ https://t.co/2c6oP1y01e ',\n",
       " ' I liked a @YouTube video https://t.co/Dp1AKu3A9n CBD Oil For Pain Relief (Vape Shot vs. Gold Label) ',\n",
       " ' What Is Juuling And Is It Really That Bad For Your Health?  https://t.co/BtP93H3pbt via @WomensHealthMag ',\n",
       " ' FYI: # ecig # vapecommunity # vapeon # vapefam Vape Dinner Lady - Apple Pie -  Blackberry Crumble & Orange Tart: Find out what I think of the three new flavours from Vape Dinner Lady; Apple Pie -  Blackberry Crumble & Orange ... https://t.co/Bf15tAoNax # TheDevilVaper # Vape # vapingAuction https://t.co/ZxfOF3Td33 ',\n",
       " ' You know ur having a rough day when u lose ur juul ',\n",
       " ' \"Remember to keep your e-juice out of direct sunlight and hot rooms- You don\\'t want the flavor to go bad!\" _newline_ . _newline_ . _newline_ . _newline_ # vapeTip # vapeon # vaping # instavape # vapestagram # cloudchaser # vapepics # vapedaily # vapefriends # vapeshop # VapeStrong # Vapingisnotacrime # VapeAllDay # BlackhouseVapor https://t.co/osOWG7QeLD ',\n",
       " \" There's rules about who this fella can be. Extra job qualifications you could say. First of all -  he vapes \",\n",
       " \" I was so hungry today tht I walked into someone 's fruity vape cloud and my mouth started watering :/ \",\n",
       " ' Lil Vape | HOT TAKES Interview: https://t.co/O3nctuqo2S via @YouTube ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam TigerTek Springer X RDA - Springy posts are springy...: The TigerTek Springer X RDA is a new dropper from a new company who has instantly went down that not so used path of spring ... https://t.co/pIkYBUmCMM # VapingWithVic # Vape # vapingAuction https://t.co/H8HYx6Al6T ',\n",
       " ' Retweeted Vape Bargains UK (@VapeBargainsUK): _newline_  _newline_   _emoj_thought_balloon_ \\n  _emoj_thought_balloon_ \\n  _emoj_thought_balloon_ \\n # ELIQUID # GIVEAWAY   _emoj_thought_balloon_ \\n  _emoj_thought_balloon_ \\n  _emoj_thought_balloon_ \\n  _newline_  _newline_ TWO lucky winners will receive the... https://t.co/rraWMf2Drn ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Watts UP?! - Ep 138 - LOTS of reviews coming up + first looks and GIVEAWAY: In this weeks Watts UP?! News on a run of double reviews coming up next week -  the shift into the autumn ... https://t.co/uw9AsVsCeG # VapingWithVic # Vape # vapingdays https://t.co/IYhH7JuDdI ',\n",
       " ' Strong Tobacco Laws May Weed Out Vapers -  Too https://t.co/PFccFzZ8Mq # RN... https://t.co/ykzo37plK9... https://t.co/3RZ91YhX1M ',\n",
       " ' I liked a @YouTube video https://t.co/Y5hyGZN7YE Jordan Peterson Takes a Fat Vape Hit On the H3 Podcast ',\n",
       " ' New Product Alert! CBD 25:1 600mg Vape Cart and a new Hybrid Pineapple Express 600mg vape cart. Available everywhere today! # trulievers https://t.co/LihpYKmbJm ',\n",
       " ' I liked a @YouTube video https://t.co/JQQgm6SOKH VAPE NAYSH 4 LYFE | Prop Hunt # 50 ',\n",
       " ' Colorful Owls Padded Pipe Pouch by https://t.co/GRpeEhB0n3 -  https://t.co/CJg2W0uYU3  # stash # bags # case # accessories # vaping # smoking # purse # clutch # cozy # mmj # waxpen # chronic # handpipes # weedpipes # glasspipe # stonerstuff # shopsmall # stayhigh # cannabisculture # stonernation ',\n",
       " ' Order Puller / Packer https://t.co/WMWujMRJ8O Vape Society Supply _newline_  Location : Las Vegas NV US _newline_ *Job Summary* Order Picker Position Available for a Vape Website in Las Vegas. Previous Experience Pulling and packing Required! If you fit the below... _newline_  _newline_ More >> _newline_  _newline_  jobs by   Vap... https://t.co/EGobXIadVR ',\n",
       " ' My channel (The Department of Inhaled Vapours) has been nominated for reviewer of the year   _emoj_grinning_ \\n a cheeky Lil vote would be much appreciated # vape # vaping # vaper # vapefam # vapecommunity # vapelove # vapeporn # vapelife # vapelyfe # notblowingsmoke https://t.co/y74EDJlL9S ',\n",
       " ' @KryozGaming Ur vape machine ',\n",
       " ' Denver Approves First Social Marijuana License -  Allowing Vaping and Edibles in Lincoln Park Coffee Shop _newline_  _newline_ https://t.co/LFJPU3gY0j _newline_  _newline_ # ABL # Beer # Wine # Spirits # Marijuana https://t.co/1RTv66Qnfe ',\n",
       " ' Mango pods included? https://t.co/gylZUbaFdD ',\n",
       " ' should i buy a juul ',\n",
       " \" @Sarah_Says__ You can do is baby girl!!!! It was hard but I did it with a vape too. Try getting a juice that has menthol in it like my fav is kiwi menthol it 's great! https://t.co/TYM0dAPt94 \",\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Wotofo Serpent SMM Review + Coiling and wicking - Now the single coil RTA war begins...: The Wotofo Serpent SMM (Suck My Mod) is the latest tank from Wotofo which carries on the ... https://t.co/eXxPPAQiVA # VapingWithVic # Vape # vapingdays https://t.co/2SXgsT4VWc ',\n",
       " \" I'm so tired of seeing smoking and vape ads EVERY SINGLE YOUTUBE VIDEO I WATCH!!! \",\n",
       " ' Hey everybody as you can see i have started making vape videos i will continue making them but if you would like to have me try some type of vape trick just follow me and send me a tweet saying what you would like me to try and ill reply to it and do the video when i post next. ',\n",
       " \" Drug Trend: It 's Just a Vape Pen -  Right? https://t.co/q0HD9bgqwM \",\n",
       " \" # ecig # vapecommunity # vapeon # vapefam SmokJoy Cfiber Plus - A return to the tubular style: The SmokJoy Cfiber Plus is essentially SmokJoy's take on the CF style mods which Aspire produced at the beginning of 2016. A ... https://t.co/hhfTBWR9s6 # VapingWithVic # Vape # vapingdays https://t.co/1VkIbtoQyR \",\n",
       " ' And -  you know -  be used as cannon fodder... https://t.co/x5FeRSt8jQ ',\n",
       " \" there 's so much fog outside because of @mbolan3 vape \",\n",
       " ' 15% Off # Pax3 Complete Portable # Vaporizer # Sale Now @ https://t.co/KPrEnisJAD # Vape # Vapepen # HighTimes # Cannabis ',\n",
       " ' FYI: # ecig # vapecommunity # vapeon # vapefam I VG - 4 Flavour Review Pt.2: A review of Neon Lime -  Neon Orange -  Pineapple Blast and Summer Blaze by I VG For more information visit: *This product was sent free for the purpose ... https://t.co/P4qo4M5xAV # TheDevilVaper # Vape # vapingdays https://t.co/A56NVVPVgB ',\n",
       " ' Kids be in the gas station like  \"one pack of juul pods please \" https://t.co/wVVqXsGuvt ',\n",
       " \" please contribute to my international shipping of juul pods fund. they don't sell them here and i'm dying. help the less fortunate.  _newline_ https://t.co/Aol0FxPemN \",\n",
       " ' I want to find the vape boi to my vape girl ',\n",
       " ' Vapour Freaks: Hannibal On Ice Juice Review!!   _emoj_en_dash_\\n Empire Vape Co https://t.co/K56TGtv8E4<br><br> https://t.co/TaW4vuYkN9 ',\n",
       " ' # ecig # vapecommunity # vapeon # vapefam Coil Art Azeroth RTA + Coiling and Wicking Tutorial - o..m..g... the flavour...: The Coil Art Azeroth RTA is a rebuildable tank atty which has been making a name for itself with the ... https://t.co/cgsk1HyLrI # VapingWithVic # Vape # vapingdays https://t.co/pnjBXHKn9s ',\n",
       " ' Crazy # vape deals at https://t.co/EkGNG3iSnx and take another 10% off those deals with this # vapingtruth coupon code vapingtruth. ',\n",
       " \" FYI: # ecig # vapecommunity # vapeon # vapefam The Devil's Vlog 28/1/18 |Announcements|Patreon|Monthly Roundup|: The first of many VLOGS this year! In this week I chat about the vlog -  Patreon -  what I've been up to this month ... https://t.co/plLM3fVpUm # TheDevilVaper # Vape # vapingdays https://t.co/lLCn74FA6E \",\n",
       " \" @mateoseda23 I 've lost my juul 8 times today. \",\n",
       " \" please don 't start vaping tide to go sticks or something because i really can 't have those discontinued too \",\n",
       " \" @abbystrobelt Bet I 'll bring my juul \",\n",
       " \" I can 't find my juul :( \",\n",
       " ' Vaping  _newline_ [VAE   _emoj_bullet_\\n peeng]  _newline_ (Noun)  _newline_  _newline_ definition : when you must suck on something -  but a dick is not available. ',\n",
       " \" @NathanieIHall @YouTube @TeamYouTube @YTCreators Good lord. This is getting stupid. Their AI for flagging stuff is ridiculous. Unless there's a few people reporting it just to be Dicks. Can't even vape now unless you're sucking @YouTube collective balls like the top tier vape and weed tubers are. Ugh \",\n",
       " \" # ecig # vapecommunity # vapeon # vapefam AugVape V200 Review - AugVape sticking with the basics...: In today's review -  we are having a look at the AugVape V200. If you look at a lot of the past AugVape mods -  you'll notice a ... https://t.co/n7P2BQFBIC # VapingWithVic # Vape # vapingdays https://t.co/I7gBCDZr59 \",\n",
       " \" What role can tobacco and pharmaceutical industries have in a # vaping future? Join this discussion with our panel of experts led by MP Gareth Johnson at UKVIA's Vaping Industry Forum - in London -  23 April. Book tickets here: https://t.co/Fk1WDl3dFl # UKVIAforum2018 # goingforgrowth https://t.co/HcsFrQAA7b \",\n",
       " \" # ecig # vapecommunity # vapeon # vapefam Asmodus Voluna RTA Review - VERY good flavour -  but with a niggling issue: In today's review -  we are having a look at the Asmodus Voluna RTA. The Voluna was released at the same time ... https://t.co/ggHDWxNo21 # VapingWithVic # Vape # vapingdays https://t.co/BFYcG4PzT7 \",\n",
       " \" We get it you vape. Just don 't blow that gay shit around here lmao \",\n",
       " ' @Innokintech Big Box Atlas  _newline_  _newline_ # vape # vapes # innokin # atlas # bigbox # bigboxatlas # ecig # quitsmoking # notblowingsmoke https://t.co/4c2YjAY6xe ',\n",
       " ' yugyeom _newline_ - aka Yasir _newline_ - says  \"wallah \" every 10 seconds _newline_ - pretty sure he lives inside of a hookah bar _newline_ - doesnt eat pork but smokes weed & gets drunk _newline_ - mama \\'s boy _newline_ - hit his juul in the parking lot ',\n",
       " \" Vape and she won't fuck ya # TheNextVersionOfNetflixAndChill \",\n",
       " ' FYI: # ecig # vapecommunity # vapeon # vapefam Max VG - Grape Bubbles & Blue Bubbles Review: A review of Grape Bubbles and Blue by Max VG For more information visit: https://t.co/pJJnncN41q DISCOUNT CODE: VAPE30 My ... https://t.co/WqdiA71mPH # TheDevilVaper # Vape # vapingAuction https://t.co/D5IpXPpel1 ',\n",
       " \" @luckyk1tty i didn 't know if that was a vape or boxed alcohol lol \",\n",
       " ' First re-wrap ever. Did I do good? via /r/Vaping https://t.co/uWDBz6Qy4K # vaping # vapers ',\n",
       " ' Daily Dose of Tim - Episode 107 (Burritos and a Vape): https://t.co/Yx5BGCHmY1 via @YouTube ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_all[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ecig vapecommunity vapeon vapefam first look stentorian basilisk please note this is not a review this is a first look at the product only review will be up later this month the late late uk vape show is vapingwithvic vape vapingauction',\n",
       " 'ecig vapecommunity vapeon vapefam times vape tenacioustx vapes dreamer review a solid mech mod the times vape and tenacioustxvapes dreamer mech has been creating a lot of waves over the past couple of months vapingwithvic vape vapingdays',\n",
       " 'all of my co workers including the manager r outside vaping and im alone at front counter',\n",
       " 'when i m too old to smoke i m gonna start vaping until then my lungs better do their job',\n",
       " 'kansas man sues vape shop for e cigarette battery explosion in other words man attempts to blame his stupidity on others instead of owning it lawyer vultures looking for a dollar',\n",
       " 'vapour freaks hannibal on ice juice review _emoj_en_dash_ empire vape co',\n",
       " 'ecig vapecommunity vapeon vapefam wotofo serpent smm review coiling and wicking now the single coil rta war begins the wotofo serpent smm suck my mod is the latest tank from wotofo which carries on the vapingwithvic vape vapingauction',\n",
       " 'all those vape gods my school are shook over this',\n",
       " 'vape til you die',\n",
       " 'fyi ecig vapecommunity vapeon vapefam the devil vaper s best worst of a video rounding up the year and giving my personal best and worst of please refer to the timestamps below to jump to your thedevilvaper vape vapingdays',\n",
       " 'holy fuck i underestimated the juul power hducuxjslw',\n",
       " 'my mom just caught me charging my vape lmao fuuuuck i lied straight to her face and said it was not mine _emoj_joy_ _emoj_sob_',\n",
       " 'man i got these pods in the mail wednesday and i m already almost done with my third pod no one ask me touch my juul bc i will sucker punch the shit out of you   and don t touch me either',\n",
       " 'so any chance  will be moving to correct the similar misconception that you helped create that vaping etc is worthless for risk reduction except when immediate smoking abstinence occurs ',\n",
       " 'fyi ecig vapecommunity vapeon vapefam bryn s special sauce t xc a review of bryn s special sauce t xc fresh and candied citrus fruits and rangpur are blended with notes of jasmine darjeeling and assam and thedevilvaper vape vapingauction',\n",
       " 'it is pm in timezones rootsreggae vapes  ',\n",
       " 'can i buy a juul pod from someone lmao i don t get paid till friday and i can t be spendin freakin on a whole pack _emoj_joy_ _emoj_joy_ _emoj_joy_ _emoj_joy_ _emoj_joy_',\n",
       " 'i dated a guy who looked like him',\n",
       " 'just watched a kid blow a massive vape cloud while hover boarding at mach speed across campus we are the pinnacle of technology here at rit',\n",
       " 'ecig vapecommunity vapeon vapefam watts up ep tpd aftermath expo medusa reborn giveaway in this weeks watts up the aftermath of may th vaper expo uk a lot of subscriber and viewer mail vapingwithvic vape vapingauction',\n",
       " 'ecig vapecommunity vapeon vapefam watts up ep expo news k subs first looks and big box giveaway in this weeks watts up the old expo problems pop up again an interesting vlog from chris empire vape vapingwithvic vape vapingdays',\n",
       " 'find all your ecig vape supplies at on in milwaukee helpful knowledgeable staff',\n",
       " 'fyi ecig vapecommunity vapeon vapefam the devil s vlog episode askthedevil the devil s vlog ep time stamps down below in this episode i cover what i have been vaping on this week what products have thedevilvaper vape vapingauction',\n",
       " '_emoj_black_right_pointing_triangle_  puff in style with the puffco plus the puffco plus vape pen has a sleek simple discrete design so you can inconspicuously vape concentrates on the go in style no torches no mess expect a streamlined experience the pen comes in',\n",
       " 'wholesale vjuice  ecigs vapecommunity vapefamily vapenation vapeshop vapetricks cloudchaser vapelife vapeporn ecigs vapenation vape cloudchaser vapelife jbrt  wholesale sales fast dpd delivery  e mail sms co uk or call paul on',\n",
       " 'ecig vapecommunity vapeon vapefam vaptio c kit review one of the larger pen style kits in today s review we are having a look at the vaptio c kit the latest of the pen style kits from vaptio the cii takes vapingwithvic vape vapingdays',\n",
       " 'plays ultimate frisbee  suspended for vaping in bathroom  works at little caesars  wears mandals to school',\n",
       " ' found a juul charger on the ground in collegetown this weekend life has been good',\n",
       " 'that moment you decide to run low wattage and you need the best to run with it so u top it off with an elite v from armageddon and some of the best juice fluffer from liquid sky  vapes vapenation vapelife vapefamily vapefam vapes vapeon vapedaily vapelyfe',\n",
       " 'ecig vapecommunity vapeon vapefam dead rabbit rda review very impressive rda great for squonking in today s review we are having a look at the dead rabbit rda by hellvape and designed by heathen in a market vapingwithvic vape vapingtrain',\n",
       " 'vapes are so loud today more than eggs whats going on   iheartawards bestfanarmy btsarmy',\n",
       " 'maybe kids are vaping it is that or drugs and alcohol _emoj_thinking_face_ whichwouldiprefer',\n",
       " ' it varies by how much pain your in i vape about mg just to be on the safe side but it ranges from mg mg and price varies too i pay around a month but i think i am gonna up my level to mg but just like vape juice the got different flavors and all',\n",
       " 'two bloody mary s down moderately buzzed and discreetly vaping in th airport toilets _emoj_fitzpatrick_type_  ',\n",
       " 'sorry vaping and edibles is not good enough  ',\n",
       " 'it all started when da vinci chan s shop got a vape section at first it was just a few like robin hood and kintoki now over half of the servants vape the c block is so filled with vape mist i cant go there without choking to death',\n",
       " 'i accidentally posted a video of me hitting a vape pen and my fam is going to think it is wax _emoj_sob_ _emoj_sob_ i done fucked up',\n",
       " 'i got told to draw marie vaping  i suck at expressions lmaooooo',\n",
       " 'here are the witnesses vaping vapinguk',\n",
       " 'can u hold my juul',\n",
       " 'no i won t fill up your kush vape for you ask someone else _emoj_face_palm_ _emoj_fitzpatrick_type_ _emoj_male_sign_',\n",
       " 'ecig vapecommunity vapeon vapefam vape mail magazine e liquid samples july subscription package having a vape on this months july e liquid samples from the vape mail magazine subscription vape mail vapingwithvic vape vapingauction',\n",
       " 'ecig vapecommunity vapeon vapefam dovpo m vv mod review dovpo go back to basics in today s review we are having a look at the dovpo m vv mod a potentiometer based mod from dovpo the version i have here is vapingwithvic vape vapingauction',\n",
       " 'russia may take a pro vaping stance  by jim mcdonald ',\n",
       " 'i liked a video cbd oil for pain relief vape shot vs gold label',\n",
       " 'what is juuling and is it really that bad for your health via',\n",
       " 'fyi ecig vapecommunity vapeon vapefam vape dinner lady apple pie blackberry crumble orange tart find out what i think of the three new flavours from vape dinner lady apple pie blackberry crumble orange thedevilvaper vape vapingauction',\n",
       " 'you know ur having a rough day when u lose ur juul',\n",
       " 'remember to keep your e juice out of direct sunlight and hot rooms you do not want the flavor to go bad     vapetip vapeon vaping instavape vapestagram cloudchaser vapepics vapedaily vapefriends vapeshop vapestrong vapingisnotacrime vapeallday blackhousevapor',\n",
       " 'there is rules about who this fella can be extra job qualifications you could say first of all he vapes',\n",
       " 'i was so hungry today tht i walked into someone s fruity vape cloud and my mouth started watering',\n",
       " 'lil vape hot takes interview via',\n",
       " 'ecig vapecommunity vapeon vapefam tigertek springer x rda springy posts are springy the tigertek springer x rda is a new dropper from a new company who has instantly went down that not so used path of spring vapingwithvic vape vapingauction',\n",
       " 'retweeted vape bargains uk   _emoj_thought_balloon_ _emoj_thought_balloon_ _emoj_thought_balloon_ eliquid giveaway _emoj_thought_balloon_ _emoj_thought_balloon_ _emoj_thought_balloon_   two lucky winners will receive the',\n",
       " 'ecig vapecommunity vapeon vapefam watts up ep lots of reviews coming up first looks and giveaway in this weeks watts up news on a run of double reviews coming up next week the shift into the autumn vapingwithvic vape vapingdays',\n",
       " 'strong tobacco laws may weed out vapers too rn',\n",
       " 'i liked a video jordan peterson takes a fat vape hit on the h podcast',\n",
       " 'new product alert cbd mg vape cart and a new hybrid pineapple express mg vape cart available everywhere today trulievers',\n",
       " 'i liked a video vape naysh lyfe prop hunt',\n",
       " 'colorful owls padded pipe pouch by stash bags case accessories vaping smoking purse clutch cozy mmj waxpen chronic handpipes weedpipes glasspipe stonerstuff shopsmall stayhigh cannabisculture stonernation',\n",
       " 'order puller packer vape society supply  location las vegas nv us  job summary order picker position available for a vape website in las vegas previous experience pulling and packing required if you fit the below   more   jobs by vap',\n",
       " 'my channel the department of inhaled vapours has been nominated for reviewer of the year _emoj_grinning_ a cheeky lil vote would be much appreciated vape vaping vaper vapefam vapecommunity vapelove vapeporn vapelife vapelyfe notblowingsmoke',\n",
       " 'ur vape machine',\n",
       " 'denver approves first social marijuana license allowing vaping and edibles in lincoln park coffee shop     abl beer wine spirits marijuana',\n",
       " 'mango pods included',\n",
       " 'should i buy a juul',\n",
       " ' you can do is baby girl it was hard but i did it with a vape too try getting a juice that has menthol in it like my fav is kiwi menthol it s great',\n",
       " 'ecig vapecommunity vapeon vapefam wotofo serpent smm review coiling and wicking now the single coil rta war begins the wotofo serpent smm suck my mod is the latest tank from wotofo which carries on the vapingwithvic vape vapingdays',\n",
       " 'i am so tired of seeing smoking and vape ads every single youtube video i watch',\n",
       " 'hey everybody as you can see i have started making vape videos i will continue making them but if you would like to have me try some type of vape trick just follow me and send me a tweet saying what you would like me to try and ill reply to it and do the video when i post next',\n",
       " 'drug trend it s just a vape pen right',\n",
       " 'ecig vapecommunity vapeon vapefam smokjoy cfiber plus a return to the tubular style the smokjoy cfiber plus is essentially smokjoy s take on the cf style mods which aspire produced at the beginning of a vapingwithvic vape vapingdays',\n",
       " 'and you know be used as cannon fodder',\n",
       " 'there s so much fog outside because of vape',\n",
       " 'off pax complete portable vaporizer sale now vape vapepen hightimes cannabis',\n",
       " 'fyi ecig vapecommunity vapeon vapefam i vg flavour review pt a review of neon lime neon orange pineapple blast and summer blaze by i vg for more information visit this product was sent free for the purpose thedevilvaper vape vapingdays',\n",
       " 'kids be in the gas station like one pack of juul pods please',\n",
       " 'please contribute to my international shipping of juul pods fund they do not sell them here and i am dying help the less fortunate ',\n",
       " 'i want to find the vape boi to my vape girl',\n",
       " 'vapour freaks hannibal on ice juice review _emoj_en_dash_ empire vape co',\n",
       " 'ecig vapecommunity vapeon vapefam coil art azeroth rta coiling and wicking tutorial o m g the flavour the coil art azeroth rta is a rebuildable tank atty which has been making a name for itself with the vapingwithvic vape vapingdays',\n",
       " 'crazy vape deals at and take another off those deals with this vapingtruth coupon code vapingtruth',\n",
       " 'fyi ecig vapecommunity vapeon vapefam the devil s vlog announcements patreon monthly roundup the first of many vlogs this year in this week i chat about the vlog patreon what i have been up to this month thedevilvaper vape vapingdays',\n",
       " 'i ve lost my juul times today',\n",
       " 'please don t start vaping tide to go sticks or something because i really can t have those discontinued too',\n",
       " 'bet i ll bring my juul',\n",
       " 'i can t find my juul',\n",
       " 'vaping  vae _emoj_bullet_ peeng  noun   definition when you must suck on something but a dick is not available',\n",
       " 'good lord this is getting stupid their ai for flagging stuff is ridiculous unless there is a few people reporting it just to be dicks cannot even vape now unless you are sucking collective balls like the top tier vape and weed tubers are ugh',\n",
       " 'ecig vapecommunity vapeon vapefam augvape v review augvape sticking with the basics in today s review we are having a look at the augvape v if you look at a lot of the past augvape mods you will notice a vapingwithvic vape vapingdays',\n",
       " 'what role can tobacco and pharmaceutical industries have in a vaping future join this discussion with our panel of experts led by mp gareth johnson at ukvia s vaping industry forum in london april book tickets here ukviaforum goingforgrowth',\n",
       " 'ecig vapecommunity vapeon vapefam asmodus voluna rta review very good flavour but with a niggling issue in today s review we are having a look at the asmodus voluna rta the voluna was released at the same time vapingwithvic vape vapingdays',\n",
       " 'we get it you vape just don t blow that gay shit around here lmao',\n",
       " 'big box atlas   vape vapes innokin atlas bigbox bigboxatlas ecig quitsmoking notblowingsmoke',\n",
       " 'yugyeom  aka yasir  says wallah every seconds  pretty sure he lives inside of a hookah bar  doesnt eat pork but smokes weed gets drunk  mama s boy  hit his juul in the parking lot',\n",
       " 'vape and she will not fuck ya thenextversionofnetflixandchill',\n",
       " 'fyi ecig vapecommunity vapeon vapefam max vg grape bubbles blue bubbles review a review of grape bubbles and blue by max vg for more information visit discount code vape my thedevilvaper vape vapingauction',\n",
       " 'i didn t know if that was a vape or boxed alcohol lol',\n",
       " 'first re wrap ever did i do good via r vaping vaping vapers',\n",
       " 'daily dose of tim episode burritos and a vape via']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = tweets_all[:100]\n",
    "result = []\n",
    "for t in testing:\n",
    "    tweet = tweet_cleaner(t)\n",
    "    result.append(remove_underscores(tweet))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "Tweets 10000 of 400000 has been processed\n",
      "Tweets 20000 of 400000 has been processed\n",
      "Tweets 30000 of 400000 has been processed\n",
      "Tweets 40000 of 400000 has been processed\n",
      "Tweets 50000 of 400000 has been processed\n",
      "Tweets 60000 of 400000 has been processed\n",
      "Tweets 70000 of 400000 has been processed\n",
      "Tweets 80000 of 400000 has been processed\n",
      "Tweets 90000 of 400000 has been processed\n",
      "Tweets 100000 of 400000 has been processed\n",
      "Tweets 110000 of 400000 has been processed\n",
      "Tweets 120000 of 400000 has been processed\n",
      "Tweets 130000 of 400000 has been processed\n",
      "Tweets 140000 of 400000 has been processed\n",
      "Tweets 150000 of 400000 has been processed\n",
      "Tweets 160000 of 400000 has been processed\n",
      "Tweets 170000 of 400000 has been processed\n",
      "Tweets 180000 of 400000 has been processed\n",
      "Tweets 190000 of 400000 has been processed\n",
      "Tweets 200000 of 400000 has been processed\n",
      "Tweets 210000 of 400000 has been processed\n",
      "Tweets 220000 of 400000 has been processed\n",
      "Tweets 230000 of 400000 has been processed\n",
      "Tweets 240000 of 400000 has been processed\n",
      "Tweets 250000 of 400000 has been processed\n",
      "Tweets 260000 of 400000 has been processed\n",
      "Tweets 270000 of 400000 has been processed\n",
      "Tweets 280000 of 400000 has been processed\n",
      "Tweets 290000 of 400000 has been processed\n",
      "Tweets 300000 of 400000 has been processed\n",
      "Tweets 310000 of 400000 has been processed\n",
      "Tweets 320000 of 400000 has been processed\n",
      "Tweets 330000 of 400000 has been processed\n",
      "Tweets 340000 of 400000 has been processed\n",
      "Tweets 350000 of 400000 has been processed\n",
      "Tweets 360000 of 400000 has been processed\n",
      "Tweets 370000 of 400000 has been processed\n",
      "Tweets 380000 of 400000 has been processed\n",
      "Tweets 390000 of 400000 has been processed\n",
      "Tweets 400000 of 400000 has been processed\n"
     ]
    }
   ],
   "source": [
    "nums = [0,400000,800000,1200000,1600000,1899851]\n",
    "print(\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweets = []\n",
    "for i in range(nums[0],nums[1]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                   \n",
    "    clean_tweets.append(tweet_cleaner(tweets_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 410000 of 400000 has been processed\n",
      "Tweets 420000 of 400000 has been processed\n",
      "Tweets 430000 of 400000 has been processed\n",
      "Tweets 440000 of 400000 has been processed\n",
      "Tweets 450000 of 400000 has been processed\n",
      "Tweets 460000 of 400000 has been processed\n",
      "Tweets 470000 of 400000 has been processed\n",
      "Tweets 480000 of 400000 has been processed\n",
      "Tweets 490000 of 400000 has been processed\n",
      "Tweets 500000 of 400000 has been processed\n",
      "Tweets 510000 of 400000 has been processed\n",
      "Tweets 520000 of 400000 has been processed\n",
      "Tweets 530000 of 400000 has been processed\n",
      "Tweets 540000 of 400000 has been processed\n",
      "Tweets 550000 of 400000 has been processed\n",
      "Tweets 560000 of 400000 has been processed\n",
      "Tweets 570000 of 400000 has been processed\n",
      "Tweets 580000 of 400000 has been processed\n",
      "Tweets 590000 of 400000 has been processed\n",
      "Tweets 600000 of 400000 has been processed\n",
      "Tweets 610000 of 400000 has been processed\n",
      "Tweets 620000 of 400000 has been processed\n",
      "Tweets 630000 of 400000 has been processed\n",
      "Tweets 640000 of 400000 has been processed\n",
      "Tweets 650000 of 400000 has been processed\n",
      "Tweets 660000 of 400000 has been processed\n",
      "Tweets 670000 of 400000 has been processed\n",
      "Tweets 680000 of 400000 has been processed\n",
      "Tweets 690000 of 400000 has been processed\n",
      "Tweets 700000 of 400000 has been processed\n",
      "Tweets 710000 of 400000 has been processed\n",
      "Tweets 720000 of 400000 has been processed\n",
      "Tweets 730000 of 400000 has been processed\n",
      "Tweets 740000 of 400000 has been processed\n",
      "Tweets 750000 of 400000 has been processed\n",
      "Tweets 760000 of 400000 has been processed\n",
      "Tweets 770000 of 400000 has been processed\n",
      "Tweets 780000 of 400000 has been processed\n",
      "Tweets 790000 of 400000 has been processed\n",
      "Tweets 800000 of 400000 has been processed\n"
     ]
    }
   ],
   "source": [
    "for i in range(nums[1],nums[2]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                   \n",
    "    clean_tweets.append(tweet_cleaner(tweets_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 810000 of 400000 has been processed\n",
      "Tweets 820000 of 400000 has been processed\n",
      "Tweets 830000 of 400000 has been processed\n",
      "Tweets 840000 of 400000 has been processed\n",
      "Tweets 850000 of 400000 has been processed\n",
      "Tweets 860000 of 400000 has been processed\n",
      "Tweets 870000 of 400000 has been processed\n",
      "Tweets 880000 of 400000 has been processed\n",
      "Tweets 890000 of 400000 has been processed\n",
      "Tweets 900000 of 400000 has been processed\n",
      "Tweets 910000 of 400000 has been processed\n",
      "Tweets 920000 of 400000 has been processed\n",
      "Tweets 930000 of 400000 has been processed\n",
      "Tweets 940000 of 400000 has been processed\n",
      "Tweets 950000 of 400000 has been processed\n",
      "Tweets 960000 of 400000 has been processed\n",
      "Tweets 970000 of 400000 has been processed\n",
      "Tweets 980000 of 400000 has been processed\n",
      "Tweets 990000 of 400000 has been processed\n",
      "Tweets 1000000 of 400000 has been processed\n",
      "Tweets 1010000 of 400000 has been processed\n",
      "Tweets 1020000 of 400000 has been processed\n",
      "Tweets 1030000 of 400000 has been processed\n",
      "Tweets 1040000 of 400000 has been processed\n",
      "Tweets 1050000 of 400000 has been processed\n",
      "Tweets 1060000 of 400000 has been processed\n",
      "Tweets 1070000 of 400000 has been processed\n",
      "Tweets 1080000 of 400000 has been processed\n",
      "Tweets 1090000 of 400000 has been processed\n",
      "Tweets 1100000 of 400000 has been processed\n",
      "Tweets 1110000 of 400000 has been processed\n",
      "Tweets 1120000 of 400000 has been processed\n",
      "Tweets 1130000 of 400000 has been processed\n",
      "Tweets 1140000 of 400000 has been processed\n",
      "Tweets 1150000 of 400000 has been processed\n",
      "Tweets 1160000 of 400000 has been processed\n",
      "Tweets 1170000 of 400000 has been processed\n",
      "Tweets 1180000 of 400000 has been processed\n",
      "Tweets 1190000 of 400000 has been processed\n",
      "Tweets 1200000 of 400000 has been processed\n"
     ]
    }
   ],
   "source": [
    "for i in range(nums[2],nums[3]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                   \n",
    "    clean_tweets.append(tweet_cleaner(tweets_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 1210000 of 400000 has been processed\n",
      "Tweets 1220000 of 400000 has been processed\n",
      "Tweets 1230000 of 400000 has been processed\n",
      "Tweets 1240000 of 400000 has been processed\n",
      "Tweets 1250000 of 400000 has been processed\n",
      "Tweets 1260000 of 400000 has been processed\n",
      "Tweets 1270000 of 400000 has been processed\n",
      "Tweets 1280000 of 400000 has been processed\n",
      "Tweets 1290000 of 400000 has been processed\n",
      "Tweets 1300000 of 400000 has been processed\n",
      "Tweets 1310000 of 400000 has been processed\n",
      "Tweets 1320000 of 400000 has been processed\n",
      "Tweets 1330000 of 400000 has been processed\n",
      "Tweets 1340000 of 400000 has been processed\n",
      "Tweets 1350000 of 400000 has been processed\n",
      "Tweets 1360000 of 400000 has been processed\n",
      "Tweets 1370000 of 400000 has been processed\n",
      "Tweets 1380000 of 400000 has been processed\n",
      "Tweets 1390000 of 400000 has been processed\n",
      "Tweets 1400000 of 400000 has been processed\n",
      "Tweets 1410000 of 400000 has been processed\n",
      "Tweets 1420000 of 400000 has been processed\n",
      "Tweets 1430000 of 400000 has been processed\n",
      "Tweets 1440000 of 400000 has been processed\n",
      "Tweets 1450000 of 400000 has been processed\n",
      "Tweets 1460000 of 400000 has been processed\n",
      "Tweets 1470000 of 400000 has been processed\n",
      "Tweets 1480000 of 400000 has been processed\n",
      "Tweets 1490000 of 400000 has been processed\n",
      "Tweets 1500000 of 400000 has been processed\n",
      "Tweets 1510000 of 400000 has been processed\n",
      "Tweets 1520000 of 400000 has been processed\n",
      "Tweets 1530000 of 400000 has been processed\n",
      "Tweets 1540000 of 400000 has been processed\n",
      "Tweets 1550000 of 400000 has been processed\n",
      "Tweets 1560000 of 400000 has been processed\n",
      "Tweets 1570000 of 400000 has been processed\n",
      "Tweets 1580000 of 400000 has been processed\n",
      "Tweets 1590000 of 400000 has been processed\n",
      "Tweets 1600000 of 400000 has been processed\n"
     ]
    }
   ],
   "source": [
    "for i in range(nums[3],nums[4]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                   \n",
    "    clean_tweets.append(tweet_cleaner(tweets_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets 1610000 of 400000 has been processed\n",
      "Tweets 1620000 of 400000 has been processed\n",
      "Tweets 1630000 of 400000 has been processed\n",
      "Tweets 1640000 of 400000 has been processed\n",
      "Tweets 1650000 of 400000 has been processed\n",
      "Tweets 1660000 of 400000 has been processed\n",
      "Tweets 1670000 of 400000 has been processed\n",
      "Tweets 1680000 of 400000 has been processed\n",
      "Tweets 1690000 of 400000 has been processed\n",
      "Tweets 1700000 of 400000 has been processed\n",
      "Tweets 1710000 of 400000 has been processed\n",
      "Tweets 1720000 of 400000 has been processed\n",
      "Tweets 1730000 of 400000 has been processed\n",
      "Tweets 1740000 of 400000 has been processed\n",
      "Tweets 1750000 of 400000 has been processed\n",
      "Tweets 1760000 of 400000 has been processed\n",
      "Tweets 1770000 of 400000 has been processed\n",
      "Tweets 1780000 of 400000 has been processed\n",
      "Tweets 1790000 of 400000 has been processed\n",
      "Tweets 1800000 of 400000 has been processed\n",
      "Tweets 1810000 of 400000 has been processed\n",
      "Tweets 1820000 of 400000 has been processed\n",
      "Tweets 1830000 of 400000 has been processed\n",
      "Tweets 1840000 of 400000 has been processed\n",
      "Tweets 1850000 of 400000 has been processed\n",
      "Tweets 1860000 of 400000 has been processed\n",
      "Tweets 1870000 of 400000 has been processed\n",
      "Tweets 1880000 of 400000 has been processed\n",
      "Tweets 1890000 of 400000 has been processed\n"
     ]
    }
   ],
   "source": [
    "for i in range(nums[4],nums[5]):\n",
    "    if( (i+1)%10000 == 0 ):\n",
    "        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                   \n",
    "    clean_tweets.append(tweet_cleaner(tweets_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899851"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pylon5/be5fpap/sanyabt/RITHM/parser_out/jan_july_2018/clean_tweets.pickle', 'wb') as outfile:\n",
    "    pickle.dump(clean_tweets, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec embeddings with tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pylon5/be5fpap/sanyabt/RITHM/parser_out/jan_july_2018/clean_tweets.pickle', 'rb') as file_i:\n",
    "    tweets = pickle.load(file_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.Series(tweets)\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec CBOW model train - unigram only\n",
    "\n",
    "Parameters:\n",
    "1. Embedding size = 100\n",
    "2. Window size = 2\n",
    "3. Min_count of words = 2\n",
    "4. Training epochs = 30\n",
    "5. Alpha = 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2556822.68it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ug_cbow = Word2Vec(sg=0, size=150, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2580541.66it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2611236.14it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2549674.13it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2288972.72it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2698154.00it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2665086.49it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2464568.91it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2652797.48it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2666535.71it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2730134.46it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2649098.76it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2496412.17it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2656492.37it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2535541.46it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2669439.76it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2593798.18it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2695493.50it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2599274.83it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2655924.82it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2629636.28it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2794313.66it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2659352.39it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2410154.81it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2655951.38it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2642308.57it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2700628.46it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2537046.22it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2643897.14it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2450203.75it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2668356.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 10s, sys: 24.5 s, total: 37min 34s\n",
      "Wall time: 10min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save('embedding_150/word2vec_cbow.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('juuls', 0.6018027067184448),\n",
       " ('jewel', 0.5215368866920471),\n",
       " ('chapstick', 0.5040896534919739),\n",
       " ('cucumber', 0.49874627590179443),\n",
       " ('phone', 0.4916815757751465),\n",
       " ('girlfriend', 0.4847780764102936),\n",
       " ('dumbass', 0.4741102159023285),\n",
       " ('laptop', 0.4717305302619934),\n",
       " ('blinker', 0.46918725967407227),\n",
       " ('blunt', 0.46792906522750854)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('juul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rip', 0.8320730328559875),\n",
       " ('smack', 0.7084908485412598),\n",
       " ('borrow', 0.6496420502662659),\n",
       " ('hits', 0.6466327905654907),\n",
       " ('hitting', 0.6198492050170898),\n",
       " ('rips', 0.6154642105102539),\n",
       " ('sip', 0.6020081639289856),\n",
       " ('shove', 0.5909634232521057),\n",
       " ('pull', 0.5908858776092529),\n",
       " ('ripped', 0.5871677398681641)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('hit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ig', 0.6586094498634338),\n",
       " ('instagram', 0.6573286056518555),\n",
       " ('insta', 0.6345866918563843),\n",
       " ('facebook', 0.6197099089622498),\n",
       " ('snapchat', 0.6184684038162231),\n",
       " ('twt', 0.5588493943214417),\n",
       " ('finsta', 0.5585911273956299),\n",
       " ('reddit', 0.5442312359809875),\n",
       " ('youtube', 0.5370053648948669),\n",
       " ('tl', 0.5360217690467834)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('youth', 0.5745497941970825),\n",
       " ('teenage', 0.5454083681106567),\n",
       " ('teens', 0.46677231788635254),\n",
       " ('student', 0.46478596329689026),\n",
       " ('epidemic', 0.4534418284893036),\n",
       " ('trend', 0.4261566698551178),\n",
       " ('child', 0.4252815842628479),\n",
       " ('teenager', 0.414495050907135),\n",
       " ('underage', 0.41388779878616333),\n",
       " ('teenagers', 0.4111449718475342)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('teen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec skipgram model train - unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2390972.74it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ug_sg = Word2Vec(sg=1, size=150, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2560350.30it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2734093.11it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2644887.02it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2632814.48it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2600494.62it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2796271.84it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2864254.14it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2649239.68it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2706488.82it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2774624.62it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2765352.02it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2709288.07it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2614955.95it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2616467.12it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2707182.11it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2742401.67it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2621033.54it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2702234.80it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2753716.26it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2737820.55it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2756190.76it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2364771.11it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2602106.37it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2796831.27it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2786670.04it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2800208.26it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2633572.36it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2747062.54it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2770839.72it/s]\n",
      "100%|██████████| 1899851/1899851 [00:00<00:00, 2787172.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 21min 10s, sys: 16.6 s, total: 1h 21min 26s\n",
      "Wall time: 10min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_sg.save('embedding_150/word2vec_sg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('juuls', 0.6332941651344299),\n",
       " ('sallisaw', 0.5805323123931885),\n",
       " ('juulip', 0.5532362461090088),\n",
       " ('juulinator', 0.5477495789527893),\n",
       " ('vickii', 0.5462276339530945),\n",
       " ('thxxxxx', 0.5436975955963135),\n",
       " ('oneof', 0.5336681008338928),\n",
       " ('kaid', 0.531792163848877),\n",
       " ('_ansuryan', 0.5306997299194336),\n",
       " ('lipsss', 0.5270533561706543)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_sg.most_similar('juul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rip', 0.7213974595069885),\n",
       " ('hits', 0.6625513434410095),\n",
       " ('smack', 0.6397694945335388),\n",
       " ('borrow', 0.5885465741157532),\n",
       " ('hitting', 0.584056556224823),\n",
       " ('bathroon', 0.583244264125824),\n",
       " ('pleaz', 0.5803359746932983),\n",
       " ('bunkmate', 0.5573672652244568),\n",
       " ('pleeaaseee', 0.5569705367088318),\n",
       " ('lwmkfkfkfkfkkfkfkflhlflgk', 0.5565747022628784)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_sg.most_similar('hit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('flavourbossuk', 0.6281245946884155),\n",
       " ('instagram', 0.6255084276199341),\n",
       " ('insta', 0.6050515174865723),\n",
       " ('pizzabottle', 0.6037068367004395),\n",
       " ('psvxfacbgj', 0.5976096391677856),\n",
       " ('_clout', 0.5944842100143433),\n",
       " ('olyvaporworks', 0.5920782089233398),\n",
       " ('promoteyoutube', 0.5849149227142334),\n",
       " ('jrlnkebpi', 0.5764425992965698),\n",
       " ('facebook', 0.5750778317451477)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_sg.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vectors and concatenate to give 200 dimensional vector for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('embedding_150/word2vec_cbow.model')\n",
    "model_ug_sg = KeyedVectors.load('embedding_150/word2vec_sg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_ug_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save embeddings\n",
    "with open('embedding_150/word2vec_unigram_combined.pickle', 'wb') as file_embed:\n",
    "    pickle.dump(embeddings_index, file_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec phrase model - include bigrams in both CBOW and skipgram\n",
    "\n",
    "(Other parameters same as unigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count=5, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=2,\n",
    "                     negative=5,\n",
    "                     window=2,\n",
    "                     size=150,\n",
    "                     alpha=0.065, \n",
    "                     min_alpha=0.065,\n",
    "                     workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 1.59 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 52.84 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sourin', 0.43230533599853516),\n",
       " ('breakfast_lunch', 0.409820020198822),\n",
       " ('inhaler', 0.3965938687324524),\n",
       " ('quesadilla', 0.3898640275001526),\n",
       " ('dropkit', 0.3821324110031128),\n",
       " ('flash_drive', 0.37918415665626526),\n",
       " ('inlost', 0.37634336948394775),\n",
       " ('syste', 0.3712458908557892),\n",
       " ('portable_charger', 0.36969733238220215),\n",
       " ('jagerbombs', 0.36863893270492554)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar('juul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('embedding_150/word2vec_bg_cbow.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_bg_sg = Word2Vec(sg=1,\n",
    "                    min_count=2,\n",
    "                     negative=5,\n",
    "                     window=2,\n",
    "                     size=150,\n",
    "                     alpha=0.065, \n",
    "                     min_alpha=0.065,\n",
    "                     workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 1.49 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "w2v_bg_sg.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 52.66 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_bg_sg.train(sentences, total_examples=w2v_bg_sg.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_bg_sg.save('embedding_150/word2vec_bg_sg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('justbro', 0.5578741431236267),\n",
       " ('bro_please', 0.5501075983047485),\n",
       " ('rip', 0.5482180714607239),\n",
       " ('phat_rip', 0.5365564823150635),\n",
       " ('promose', 0.5363010168075562),\n",
       " ('one_timmeeeee', 0.5326591730117798),\n",
       " ('fix_bro', 0.5315108299255371),\n",
       " ('remwmber', 0.5282833576202393),\n",
       " ('borrow', 0.52227383852005),\n",
       " ('_elalami', 0.5201666355133057)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_bg_sg.most_similar('hit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine bigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_bg_cbow = KeyedVectors.load('embedding_150/word2vec_bg_cbow.model')\n",
    "model_bg_sg = KeyedVectors.load('embedding_150/word2vec_bg_sg.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_ug_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_bg = {}\n",
    "for w in model_bg_cbow.wv.vocab.keys():\n",
    "    embeddings_index_bg[w] = np.append(model_bg_cbow.wv[w],model_bg_sg.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save embeddings\n",
    "with open('embedding_150/word2vec_bigram_combined.pickle', 'wb') as file_embed2:\n",
    "    pickle.dump(embeddings_index_bg, file_embed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('vape_bar', 0.7845531105995178),\n",
       " ('vape_life_parovarcustomvape', 0.7380775213241577),\n",
       " ('vapestationrussia', 0.7064079642295837),\n",
       " ('vapelife_vapeforlive', 0.7003614902496338),\n",
       " ('custom_vapeparts', 0.6982174515724182),\n",
       " ('buycapeoil_onlinecbdoil', 0.6335662603378296),\n",
       " ('cravevapes', 0.6272263526916504),\n",
       " ('haugb_xf', 0.6172439455986023),\n",
       " ('vapestationrussia_vapecommunity', 0.6148149371147156),\n",
       " ('very_very_vape', 0.5990586280822754)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_bg_sg.most_similar('vape_shop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pods', 0.6260148286819458),\n",
       " ('forist', 0.5223135352134705),\n",
       " ('charger', 0.49703001976013184),\n",
       " ('closedpod', 0.46530017256736755),\n",
       " ('julep', 0.4650610387325287),\n",
       " ('phix', 0.46202394366264343),\n",
       " ('yaclut', 0.4532976746559143),\n",
       " ('cartridge', 0.4521150290966034),\n",
       " ('demandan', 0.4488767087459564),\n",
       " ('podge', 0.4307107925415039)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('pod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pod', 0.626014769077301),\n",
       " ('forist', 0.5551401972770691),\n",
       " ('chargers', 0.5112075209617615),\n",
       " ('cartridges', 0.4846333861351013),\n",
       " ('julips', 0.48204678297042847),\n",
       " ('gos', 0.46834009885787964),\n",
       " ('carts', 0.4624638557434082),\n",
       " ('shells', 0.4553850591182709),\n",
       " ('oreos', 0.4523954689502716),\n",
       " ('vonearl', 0.4454173445701599)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('pods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('thechallengexxx', 0.7865304946899414),\n",
       " ('pufffffff', 0.7846342325210571),\n",
       " ('partnerwanted', 0.7763417959213257),\n",
       " ('_emoj_wind_blowing_face_', 0.7755203247070312),\n",
       " ('freebrxvn', 0.7680383920669556),\n",
       " ('mmxii', 0.7558403611183167),\n",
       " ('evox', 0.7524769902229309),\n",
       " ('vapetesla', 0.7509241104125977),\n",
       " ('bluntugly', 0.7381649017333984),\n",
       " ('_tg', 0.7282710075378418)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('_emoj_dash_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dailytweet', 0.8055098056793213),\n",
       " ('seshhh', 0.7885046601295471),\n",
       " ('partnerwanted', 0.7725472450256348),\n",
       " ('connectsesh', 0.771475613117218),\n",
       " ('oyats', 0.7617258429527283),\n",
       " ('evox', 0.7600874304771423),\n",
       " ('tothetop', 0.7587347626686096),\n",
       " ('thechallengexxx', 0.7572304010391235),\n",
       " ('gameofthedaywinner', 0.7520869374275208),\n",
       " ('mobliquidlabs', 0.7393802404403687)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_sg.most_similar('_emoj_dash_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_emoj_dash_', 0.7755203247070312),\n",
       " ('connectsesh', 0.6801813840866089),\n",
       " ('ridgeforlife', 0.6672093272209167),\n",
       " ('mmxii', 0.6649066805839539),\n",
       " ('freebrxvn', 0.6639013290405273),\n",
       " ('thechallengexxx', 0.6637577414512634),\n",
       " ('pufffffff', 0.645920991897583),\n",
       " ('planetavapeo', 0.6434254050254822),\n",
       " ('calicrushergrinders', 0.6321915984153748),\n",
       " ('_tg', 0.6208866834640503)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('_emoj_wind_blowing_face_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sanyabt/.conda/envs/nlp_env/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rexambassador', 0.4956991374492645),\n",
       " ('_emoj_latin_ve_', 0.4739377498626709),\n",
       " ('navalisty', 0.45488712191581726),\n",
       " ('via', 0.45333296060562134),\n",
       " ('prost', 0.4365857243537903),\n",
       " ('recenze', 0.42468711733818054),\n",
       " ('review', 0.42369547486305237),\n",
       " ('vapejournal', 0.41930273175239563),\n",
       " ('vapertube_italia', 0.40626704692840576),\n",
       " ('perfrom', 0.40563878417015076)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_cbow.most_similar('_emoj_en_dash_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vocabulary of tweets used for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899851"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31071566"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = []\n",
    "for tweet in tweets:\n",
    "    tokens = tokenize.word_tokenize(tweet)\n",
    "    vocab.extend(tokens)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343573"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(vocab))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
